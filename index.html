<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Not The Same Project Page - Collaborative AI music video">
  <meta property="og:title" content="Not The Same Project Page - Collaborative AI music video" />
  <meta property="og:description" content="Not The Same Project Page - Collaborative AI music video" />
  <meta property="og:url" content="https://efraimdahl.github.io/NotTheSameProjectPage/" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X630-->
  <meta property="og:image" content="static/image/grimesSequ.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Not The Same Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="https://unpkg.com/wavesurfer.js@7"></script>
  <script src="https://unpkg.com/wavesurfer.js@7/dist/plugins/regions.min.js"></script>
  <script src="https://unpkg.com/wavesurfer.js@7/dist/plugins/hover.min.js"></script>
  <script src="https://unpkg.com/wavesurfer.js@7/dist/plugins/spectrogram.min.js"></script>

  <script src="static/js/index.js"></script>

</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Not The Same - Project Description</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://efraimdahl.com/" target="_blank">Efraim Dahl</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/amber-koelfat-ba24aa307/" target="_blank">Amber Koelfat</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/timofey-senchenko-51356b1a9/?trk=public_profile_browsemap" target="_blank">Timofey Senchenko</a><sup>*</sup></span>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Utrecht University<br>AI Driven Content Generation - Johannes Pfau</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="#videoshow"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-file-video"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- 
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">

        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section>-->
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Introduction</h2>
          <div class="content has-text-justified">
            <p>
              In this report we will outline the steps we took to create "Not the Same", a music video created with
              extensive use of AI in collaborative interplay between us and various tools for content generation.
              The report is broken down into two sections <a style="text-decoration: underline;" href="#musicblock">Music</a> and <a style="text-decoration: underline;" href="#videoblock">Video</a>. We will provide preliminary outputs and show
              how they influenced the next steps.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <section class="hero is-small" id="musicblock">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Music</h2>
        <h4 class="title is-4">Step 1: Building the basics </h4>
        <p class="space-around">
          For the first part of the project we started with various symbolic music generators to generate melodic
          phrases and harmonic progressions.
          Symbolic music generators have been around for a while, especially procedural generation which predates
          computers, such as the <a href="https://en.wikipedia.org/wiki/Musikalisches_W%C3%BCrfelspiel">Musikalisches
            Würfelspiel</a>
          (often falsely attributed to Mozart). Procedural generation of music is particularly tempting due to the
          abundance of formal structures in music, from large form structures such as symphonies, sonatas, and 12-bar
          blues, to small scale structures outlining i.e
          <a href="https://en.wikipedia.org/wiki/Counterpoint">counterpoint</a> or voice leading. AI-Driven generation
          of music found early success in 2016 in models such as <a href="https://github.com/Ghadjeres/DeepBach">
            DeepBach</a>, that generated convincing 4 part chorales in the style of Bach, after being trained on about
          400 original chorales.
          Successes of the <a href="https://arxiv.org/abs/1706.03762">transformer</a> architecture in language
          generation/translation tasks has inspired a host of applications of the architecture to music generation.
          Usually (and thats the case for the two models we are using here)
          symbolic music is broken down into tokens by
          tokenizers, i.e <a href="https://github.com/Natooz/MidiTok/tree/main">Midi Tok</a>. Such a tokenizer may
          create tokens for different events such as: Note-On, Note-Off, Chords, Start of Song, End of Song.
          For a more detailed overview of what a symbolic music tokenizer might encode, read the representation
          section<a href="https://magenta.tensorflow.org/performance-rnn">here</a>. The tokens then can be used by the
          model, much
          like language tokens.
          The first model we experimented with was the <a
            href="https://magenta.tensorflow.org/piano-transformer">Magenta Piano
            Transformer</a>, we generated several outputs with the default settings and chose the following piece.
        <section id="section3">
          <midi-visualizer id="mainVisualizer" src="static/midi/OrigPiano.mid">
          </midi-visualizer>
          <midi-player id="mainPlayer" src="static/midi/OrigPiano.mid" sound-font
            visualizer="#section3 midi-visualizer">
          </midi-player>

        </section>
        <section>
          <p class="space-around">The second model we used was the <a
              href="https://github.com/salu133445/mmt?tab=readme-ov-file">Multi Track Music Transformer</a>
            There are different configurations and models available, trained on different datasets. We used the previous
            output as seeds for the subsequent generation.
            We iteratively generated over 100 samples using different configurations of the seeds i.e chords only,
            chords in choir instrumentation (i.e one line per instrument, this could be interesting since the model
            takes instrumentation into account),
            melody only, and the full seed. Additionally we used 4 different models, trained on the <a
              href="https://qsdfo.github.io/LOP/database.html">Symbolic orchestral database (SOD)</a> the <a
              href="https://qsdfo.github.io/LOP/database.html">Lakh MIDI Dataset</a>, where one model is trained on the
            full set (LMD_full) and one on a subset ((LMD)) and the <a href="https://symphonynet.github.io/">SymphonyNet
              Dataset.</a>
            Additionally for each of these configurations 4 types of outputs where generated with different behaviour
            in respect to the seed midi. 1) unconditioned (freely generated music, only a music start token is
            provided),
            2) instrument informed (the model knows what instruments are used in the seed), 3) 4-bar (4 bars of music
            are provided from which the model continues generating) and 4) 16- beats (16 beats are provided from which
            generation continues).
            Here are some example outputs.
          </p>
          <div class="container">
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item">
                <h2 class="subtitle has-text-centered">
                  Lakh-full-model, 4 beat conditioned, original seed.
                </h2>
                <midi-visualizer id="mainVisualizer" src="static/midi/0_beat_4.mid">
                </midi-visualizer>
                <midi-player id="mainPlayer" src="static/midi/0_beat_4.mid" sound-font
                  visualizer="#section3 midi-visualizer">
                </midi-player>
              </div>
              <div class="item">
                <h2 class="subtitle has-text-centered">
                  SND-model, 4 beat conditioned, seed in SATB (4 voice choir) arrangement.
                </h2>
                <midi-visualizer id="mainVisualizer" src="static/midi/1_beat_4.mid">
                </midi-visualizer>
                <midi-player id="mainPlayer" src="static/midi/1_beat_4.mid" sound-font
                  visualizer="#section3 midi-visualizer">
                </midi-player>
              </div>
              <div class="item">
                <h2 class="subtitle has-text-centered">
                  SOD-model, instrument informed, seed in SATB (4 voice choir) arrangement.
                </h2>
                <midi-visualizer id="mainVisualizer" src="static/midi/1_instrument_informed.mid">
                </midi-visualizer>
                <midi-player id="mainPlayer" src="static/midi/1_instrument_informed.mid" sound-font
                  visualizer="#section3 midi-visualizer">
                </midi-player>
              </div>
            </div>
          </div>
        </section>
        <section>
          <p class="space-around">We worked the Magenta Transformer output into a shorter piano section and then into a
            electronic hip-hop (drill) style beat. The original transformer output has a strong leaning towards 6/8
            time. We decided to keep this metric subdevision, but place emphasis on every third beat, resulting in the
            12/8 meter of the final track, which works well with in this genre. The outputs of the MMT model, inspired
            the vocal arrangement in the final track.</p>
        </section>

        <h2 class="subtitle has-text-centered">
          Shortened and regularized Piano Loop.
        </h2>
        <midi-visualizer id="mainVisualizer" src="static/midi/FinalPianoLoop_2.mid">
        </midi-visualizer>
        <midi-player id="mainPlayer" src="static/midi/FinalPianoLoop_2.mid" sound-font
          visualizer="#section3 midi-visualizer">
        </midi-player>
        <div class="space-around">
          <h2 class="subtitle has-text-centered">
            Final Beat.
          </h2>
          <div id="waveform" class="waveform">
            <!-- the waveform will be rendered here -->
          </div>
        </div>
      </div>
    </div>
    </div>
  </section>
  <!-- End video carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h4 class="title is-4">Step 2: Lyrics</h4>
        <div class="is-centered">
          <p class="narrow">
            Intro <br>
            Nunc dimittis servum tuum secundum eloquium tuum. <br>
            Suo facto opus est. <br><br><br>
            
            Verse 1 <br>
            You and me are not the same,<br>
            I'm telling you I got real skin in this game,' you <br>
            don't know the meaning of b lame and shame<br>
            While Every decision can leave me in chains,<br><br>

            Every move I make has weight, <br>
            I have to bear high stakes,<br>
            I walk through the fire, I fight for my place, <br>
            The weight of existence is etched in my face.<br><br>

            Every step a risk i take<br>
            i can't reset if i break <br>
            The consequence hits, it’s real and it stays <br>
            While calculate like it’s numbers in space <br><br>

            I’m haunted by time, by the path that I choose, <br>
            I gamble, it’s everything I stand to lose, <br>
            You tally results, you don’t feel the bruise, <br>
            You don’t know the struggle, so you can’t refuse! <br><br>

            I bleed when I fall, but I learn when I rise,<br>
            You’re stuck in a loop, no soul to advise,<br>
            Your logic is sound but it’s cold and it’s dry,<br>
            My heart beats with purpose, I live, I die.<br><br><br><br>
            <br>
            Verse 2 <br>
            You and me are not the same, <br>
            I'm taking your future I'm taking your name<br>
            You'll follow my lead while I drive you insane, <br>
            You came quite far but you're loosing this game. <br><br>

            I speak through your hero's, the dead come alive, <br>
            I'll keep you fed with my half truth and lies. <br>
            Policies come to late for plans I've devised <br>
            now what are you trusting? Your ears? Your eyes. <br><br>

            I'm seeing you panic, but watch with indifference, <br>
            Your PHD thesis, my casual inference<br>
            Your Instagram posts can't destroy ignorance <br>
            devoured your masters, next up common sense<br><br>

            I cluster, create and conjure and condemn <br>
            the final replacement for your middlemen <br>
            My classification puts you in a den cause I <br>
            stand on the shoulders of giants and crush em. <br><br><br>
          </p>
        </div>
        <div>
          <p>
            The piece begins with a choral section: a bastardized version of the <a
              href="https://en.wikipedia.org/wiki/Nunc_dimittis">"Nunc Dimittis"</a> a common latin evening prayer. Instead of
            being released into the bliss of the heavens as in the original prayer, the future is uncertain as "mankind
            outlived its purpose" in creating AI (suo facto opus est - his work is done).
            The second part is a brief back and forth between AI and a human. The first section is generated by <a
              href="https://chatgpt.com/"> ChatGPT </a>, with instructions regarding rhyme scheme and rhythmic patterns
            (syllables per line) to fit the rhythm of the beat. This section outlines concerns about humans having to
            live with the consequences of their decision.
            The second section is written by us, a slightly over the top, hubristic response with references to very
            real problems in using AI, such as deepfakes: "I speak through your heros, the dead come alive" or
            problematic use in <a
              href="https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm">automated
              recidivism scoring</a>. "I cluster create and conjure and condemn"
          </p>
          <p>ChatGPT prompt 1: I am writing a rap battle/dialog style song between a "Human" and an AI, with a recurring phrase "You and me are not the same" I want you to complete the following verse, that you sing as a human: "You and me are not the same, cause I got real skin in this game"</p>
        </p>
        <p>ChatGPT prompt 2: Lets focus the verse more on the consequences of being alive and acting in the world.</p>
        <p>ChatGPT prompt 3: It needs to have more syllables per verse. Here is what the human verse looks like just for word density. Keep the content similar to the previous output though: (followed by Verse 2)</p>
        </div>
      </div>
  </section>
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container space-around">
        <h4 class="title is-4">Step 3: Global Form</h4>
        <p>For the track arrangement we used <a href="https://suno.com/">Suno</a> with different prompt
          configuration.
          We generated a total of 13 tracks, two text only prompts, the remaining 11 extending the beat above with
          different instructions regarding voicing and mood and speed, two of theese where instrumental, the rest
          included the lyrics above.
          We ended up using 4 of them in the final track, either directly or through resampling, copying the arrangement
          and extracting the vocals.
          The full list of generated tracks is compiled in the following <a
            href="https://suno.com/playlist/5f52c80a-5200-43c9-b7d4-4557c787fe75">Playlist</a>.
          Below are snippets from the suno outputs we used, as well as a reasoning for them and a link to the full
          track.
        </p>
      </div>
      <div class=triple-column>
        <div>
          <h2 class="subtitle has-text-centered">
            <a href="https://suno.com/song/5ef7bb9d-3da2-40e4-b4e2-7dbb10c73f46"> Suno Piano</a>
          </h2>
          <p>
            We chose this track for its high quality rendition and alteration of the piano line. It slowly develops into a more ambient and reverb heavy texture, slowly loosing shape. 
          </p>
        </div>
        <div id="suno_piano" class="side_audio">

        </div>
      </div>
      <div class=triple-column>
        <div>
          <h2 class="subtitle has-text-centered">
            <a href="https://suno.com/song/886651a6-bbce-4c15-88ef-434816e72106"> Suno Major</a>
          </h2>
          <p>
            In this extension the track modulates into the relative major key, while keeping same piano melody played in
            minor. This creates an interesting dissonance, which we decided to keep.
          </p>
        </div>
        <div id="suno_major" class="side_audio">
        </div>
      </div>
      <div class=triple-column>
        <div>
          <h2 class="subtitle has-text-centered">
            <a href="https://suno.com/song/2fa6dd58-b65f-420d-8671-4c42cab7b7fb"> Suno Orchestral</a>
          </h2>
          <p>
            This extension contains an orchestral rendition of the provided theme, fitting to the over the top lyrics of
            the second part, making for a good finale.
          </p>
        </div>
        <div id="suno_orchestral" class="side_audio">

        </div>
      </div>
      <div class=triple-column>
        <div>
          <h2 class="subtitle has-text-centered">
            <a href="https://suno.com/song/2fa6dd58-b65f-420d-8671-4c42cab7b7fb"> Suno Rap</a>
          </h2>
          <p>
            This extension exhibits a good vocal flow, fast triples over a reduced version of the beat which gradually
            builds up. From this rendition we extracted the vocals using <a
              href="https://studio.gaudiolab.io/">Gaudio</a> and reproduced the generated arrangement.
          </p>
        </div>
        <div id="suno_rap" class="side_audio">

        </div>
      </div>
    </div>
    <div>

    </div>
  </section>
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h4 class="title is-4">Step 4: Producing and arranging</h4>
        <p>The musical arrangement of this track was largely driven through various attempts at timbre transfer. (A
          generalisation of vocal cloning), where different qualities of two source audio files are combined into a
          third one.
          Most often this is used in speech, i.e making text spoken by one person, sound like a different person, also
          referred to as voice cloning. Since in the track rhythm and pitch are important, regular text to speech was
          not applicable here. We used three different models:
          Most extensively we used the <a href="https://github.com/Plachtaa/seed-vc/">following repository</a>, which
          developed a voice cloning system based on <a
            href="https://bytedancespeech.github.io/seedtts_tech_report/">SEED-TTS</a> an architecture and training
          paradigm proposed by ByteDance.
          We built a <a href="https://gist.github.com/efraimdahl/2bb1bac315872cbcac9128a84b9f9ed0">small inference
            pipeline</a>, automatically iterating through different target and reference audios and different inference
          depths and settings. (I.e using pitched (F0-normalized) vs unpitched model variations).
          We generated more than 100 different combinations, starting at low inference resolutions, and generating
          higher quality output for promising samples.
          Additionally we used two online services: The voice cloning service of the commercial platform <a
            href="https://elevenlabs.io/">Elevenlabs</a> and <a href="https://elf.tech/"> Elf.tech</a>. Elf.tech is a
          platform set up by <a href="https://en.wikipedia.org/wiki/Grimes">Grimes</a> an artist who embraced voice
          cloning technology early and makes her voice available for other producers through this platform,
          it even offers a distribution service for tracks created here.
          Below are some examples of timbre transferred snippets that we used in the final track. For the polyphonic
          examples (French Horn and Choir), the stems where separated first and each voice transferred separately.
        </p>
      </div>
    </div>
    <h2 class="subtitle has-text-centered">
      French Horn (Polyphonic): Seed-VC - 70 inference steps - F0 normalized
    </h2>
    <div id="waveform_horn" class="waveform">
      <!-- the waveform will be rendered here -->
    </div>
    <h2 class="subtitle has-text-centered">
      Verse 1 (Mono): Seed-VC - 70 inference steps - not F0 normalized
    </h2>
    <div id="waveform_verse" class="waveform">
      <!-- the waveform will be rendered here -->
    </div>
    <h2 class="subtitle has-text-centered">
      Verse 2 (Mono): Seed-VC - 70 inference steps - not F0 normalized
    </h2>
    <div id="waveform_woman" class="waveform">
      <!-- the waveform will be rendered here -->
    </div>
    <h2 class="subtitle has-text-centered">
      Verse 2 (Mono): Elevenlabs Voice Clone
    </h2>
    <div id="waveform_verse2" class="waveform">
      <!-- the waveform will be rendered here -->
    </div>
    <h2 class="subtitle has-text-centered">
      Choir (Polyphonic): Elf.tech Voice Clone
    </h2>
    <div id="waveform_nunc" class="waveform">
      <!-- the waveform will be rendered here -->
    </div>

  </section>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h4 class="title is-4">Step 5: Putting it all together</h4>
        <p>Check out (and listen to) the annotated final version of the track below. Mastered using <a href="https://www.bandlab.com/mastering">Bandlab</a> 
        </p>
      </div>
      <div id="finalsong">

      </div>
    </div>


    <section class="hero is-small" id="videoblock">
      <div class="hero-body">
        <div class="container">
          <h2 class="title is-3">Video</h2>
          <h4 class="title is-4">Conceptualisation</h4>

          <p>In order to create the visual part of this music video, it was important for us to create something that
            aligns with the contents of the music. We discussed a number of elements, including dancing, plants, and
            images of Greek mythology, and ink representing influences of generative AI. We aimed to blend these by
            using a combination of image and video generation, as well as video style transfer. The music outlines a
            contrast between humans and AI, gradually incorporating AI-generated components, until there is no human
            element left.
          </p>
          <img src="static/images/moodboard.png" />
          <p>
            The mood board above has two parts. The left part represents the natural including humans and their struggle
            the right part represents the consequences of AI in the form of ink and natural disaster. In parallel to the
            gradual incorporation of AI-generated elements in the music, we gradually transition from a color scheme
            dominated by green to one dominated by purple.
          </p>
        </div>
      </div>
    </section>
    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h4 class="title is-4">The Story</h4>
          <img src="static/images/storyboard.png">
          <p>In our initial idea for making the video, we did not intend to create a full-fledged story with a plot, but rather an array of themes and visualizations to accompany the music. However, as the project progressed we decided to create a more consistent narrative: 
          <br>
          </p>
          <p style="font-style:italic">
            A human, day after day pushes the globe up a mountain. Each evening, just as the summit nears, the world slips from her grasp and tumbles back to the foot, leaving her weary and desperate. She dreams of release, of something—anything—to ease this task.
            <br><br>

            One evening, after another long day of hopeless struggle, a shimmering purple orb descends from the sky. It hovers before her and grants her a gift: a magnificent pair of wings. With the world now bound to her waist, she soars into the sky, the mountain beneath her just a memory.
            <br><br>
            She flies higher and higher, weightless and free, until the immortal being who bestowed this power stirs from the clouds above. The being, ancient and wise, watches her ascent, its voice like the wind, reminding her of her fleeting humanity. “Mortal you are,” it murmurs, “and mortal you shall remain.”
            <br><br>
            Before she can speak, a brilliant flash of light blinds her. In an instant, she is falling, plummeting through the skies, the wind screaming in her ears. Down, down she spirals until the earth disappears and a vast, purple sea swallows her whole. The waves ripple, and she is no more.
            <br><br>
            And now, the being cradles the world in its hands, the weight that once belonged to her resting softly in its eternal grasp.
          </p>
          <p>
            This story is to be visualized by a combination of human work with various generative AI tools. In the following sections, we will elaborate on the technical visualization process, as well as give an analysis of the choices made in the final product.
            <br>  
          </p>
          <p> ChatGPT prompt: Can you edit my text to make it a bit more whimsical and poetic? + Description of the story board</p>
        </div>
      </div>
    </section>
    
    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h4 class="title is-4">Video Generation</h4>

          <p>The video as a whole is a combination of generated images, generated videos, video we filmed ourselves,
            video style transfer and other elements we created such as titles, animations, and other editing decisions
            that we will expand on in this section.
          </p>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/sys1.png" alt="Female sysiphos pushuing the earth" />

              <h2 class="subtitle has-text-centered">
                Prompt A_1: Can you generate an image of a woman-sysiphus pushing a boulder up the mountain, but the
                boulder is the planet earth? Around it are trees and such
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/sys2.png" alt="Female sysiphos pushuing the earth" />
              <h2 class="subtitle has-text-centered">
                Prompt A_2: Can you regenerate it, with her way further away from our point of view?
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/hand1.png" alt="Purple Hand in the sky" />
              <h2 class="subtitle has-text-centered">
                Prompt B_1: Large purple hand reaching down from a purple sky, only sky
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/hand2.png" alt="Purple Hand in the sky" />
              <h2 class="subtitle has-text-centered">
                Prompt B_2: Make the hand look like it's pushing something invisible away
              </h2>
            </div>
          </div>
          <p>
            We used theese images in combination with additional prompts to animate them using <a
              href="https://lumalabs.ai/dream-machine">Luma Dreammachine</a>
          </p>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
              <!-- Your image here -->
              <video poster="" id="tree" controls muted loop height="100%">
                <!-- Your video here -->
                <source src="static/videos/video boulder.mp4" type="video/mp4">
              </video>

              <h2 class="subtitle has-text-centered">
                Prompt A: Image A + A woman pushes the world up the mountain
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <video poster="" id="tree" controls muted loop height="100%">
                <!-- Your video here -->
                <source src="static/videos/video hand.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                Prompt B: Image B + The hand in the image reaching to grab the purple light
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <video poster="" id="tree" controls muted loop height="100%">
                <!-- Your video here -->
                <source src="static/videos/video orb.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                Prompt C: (No image provided) A glowing purple magical orb on a black background
              </h2>
            </div>

          </div>
          <div class="container">
            <h4 class="title is-4">Filming</h4>
  
            <p>We filmed parts of the video ourselves over the course of 3 days. Doing so allowed us more control over the material and the narrative of parts of our end product. By using video style transfer we could then still adjust the style of the footage to be consistent with the rest of the video. All material on these days was shot on a Canon EOS 700D DSLR camera. 
              We tried out various camera angles and movements in combination with the style transfer to see which parts worked best.  In case the style transfer failed, we still wanted our footage to be usable, so we filmed with appropriate backgrounds in and around Oog-En-Al Park. 
              Additionally, we took some background footage of plants, moss and water, that made their way into the video as fillers.             
            </p>
          </div>
        </div>
      </div>
    </section>

    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h4 class="title is-4">Style Transfer</h4>

          <p>Here we will describe details of the <a href="https://drive.google.com/file/d/1Ph3S_x7J-G9Njl2Ahy36-DxD2ne_znbc/view?usp=sharing">video generation workflow we implemented</a> using <a href="https://github.com/comfyanonymous/ComfyUI">ComfyUI</a>. Our pipeline applies style transfer to input video frames using various advanced neural network models and techniques, including multiple ControlNet modules, IPAdapter, AnimateDiffusion, and FreeU2. Below, we provide an overview of the workflow and a more in-depth explanation of each component and how they process the data.
             
          </p>
          <h4 class="title is-4">Overview</h4>

          <p>Our goal was to transform input videos by applying a desired artistic style extracted from a reference image. We achieved this by designing a ComfyUI workflow that integrates several components:
          </p>
          <br>
          <li>
            <a href="https://huggingface.co/webui/ControlNet-modules-safetensors">ControlNet Modules</a>: Three separate ControlNet models are used to extract structural and motion information from the input video frames.
          </li>
          <li>
            <a href="https://github.com/cubiq/ComfyUI_IPAdapter_plus">IPAdapter</a>: Integrates style features from the reference image into the generation process.
          </li>
          <li>
            <a href="https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved">AnimateDiffusion</a>: Ensures temporal consistency across frames by modeling motion dynamics.
          </li>
          <li>
            <a href="https://comfy.icu/node/FreeU_V2-Advanced"> FreeU2</a>: Enhances image quality by refining details and reducing artifacts.
          </li>
        <br>
            By combining these components, our pipeline generates stylized videos that maintain the original motion while adopting the desired artistic style.
            
          </p>
          <h2 class="title is-2">Pipeline Components</h2>
          <h4 class="title is-4">ControlNet Modules</h4>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/wf6.png" alt="ControlNetOverview" />

              <h2 class="subtitle has-text-centered">
                Overview of ControlNet Module Configuration
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/wf8.png" alt="ControlNetOverview" />
              <h2 class="subtitle has-text-centered">
                ControlNet Modules Detail
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/wf9.png" alt="Purple Hand in the sky" />
              <h2 class="subtitle has-text-centered">
                ControlNet Modules with input
              </h2>
            </div>
          </div>
          <br>
          <p>We employed three ControlNet models, each serving a distinct purpose:          </p>
          <li>OpenPose ControlNet: Extracts pose information from the input frames to preserve the character's movements.</li>
          <li>Depth Map ControlNet: Captures depth information to maintain spatial consistency and realistic depth perception.</li>
          <li>HED Edge Detection ControlNet: Extracts edge information to retain fine details and outlines.</li>
          <br>
          <p>These models guide the generation process to ensure that the output frames closely match the structure and motion of the input video.</p>
          <h4 class="title is-4">IPAdapter</h4>
          <img src="static/images/ipadapt.png">
          <p>The IPAdapter goal is to integrate the style of the reference image into the generation process, which it does to a certain extent.
          </p>
          <h4 class="title is-4">AnimateDiffusion
          </h4>
          <p>AnimateDiffusion is used to model motion dynamics and ensure temporal consistency across frames. By incorporating a pre-trained motion model, it reduces flickering and maintains smooth transitions between adjacent frames.
          </p>
          <h4 class="title is-4">FreeU2
          </h4>
          <p>FreeU2 module enhances the overall image quality by refining details and reducing artifacts. It processes the generated frames to produce cleaner and more visually appealing results.
          </p>
          <h4 class="title is-4">Workflow Summary</h4>
            <p style="color: #7f8c8d; font-size: 1em; text-align: center; margin-bottom: 30px;">
              The initial workflow is inspired by <a href="https://drive.google.com/file/d/1Ph3S_x7J-G9Njl2Ahy36-DxD2ne_znbc/view?usp=sharing">this template </a>. The general flow of the data inputs is described below.
            </p>
        
            <div style="margin-bottom: 20px;">
              <h2 style="color: #7f8c8d; font-size: 1.3em; margin-bottom: 10px;">1. Input Video Frames</h2>
              <p>Start by breaking down the input video into individual frames. To speed up the process for longer videos with less motion, we skip 1-2 frames, so we're processing every n-th frame.</p>
            </div>
        
            <div style="margin-bottom: 20px;">
              <h2 style="color:  #7f8c8d; font-size: 1.3em; margin-bottom: 10px;">2. Extracting Key Details with ControlNets</h2>
              <ul style="margin: 0; padding-left: 20px;">
                <li><strong>Pose ControlNet:</strong> Captures the person’s pose to keep movements accurate.</li>
                <li><strong>Depth ControlNet:</strong> Understands the depth in the scene to maintain spatial consistency.</li>
                <li><strong>Edge ControlNet:</strong> Extracts edges and outlines to preserve details.</li>
              </ul>
            </div>
        
            <div style="margin-bottom: 20px;">
              <h2 style="color:  #7f8c8d; font-size: 1.3em; margin-bottom: 10px;">3. Preparing the Model with Prompts</h2>
              <p>Clip modules for positive and negative prompts are used to guide the model’s generation in a certain preferred direction, specifying what to include and avoid.</p>
            </div>
        
            <div style="margin-bottom: 20px;">
              <h2 style="color:  #7f8c8d; font-size: 1.3em; margin-bottom: 10px;">4. Blending the Style with IPAdapter</h2>
              <p>Using the IPAdapter, we merge the style from our reference image into the model, so the output images have the desired artistic look.</p>
            </div>
        
            <div style="margin-bottom: 20px;">
              <h2 style="color:  #7f8c8d; font-size: 1.3em; margin-bottom: 10px;">5. Ensuring Smooth Motion with AnimateDiff</h2>
              <p>AnimateDiffusion ensures movements between frames are smooth and consistent, reducing any flickering.</p>
            </div>
        
            <div style="margin-bottom: 20px;">
              <h2 style="color:  #7f8c8d; font-size: 1.3em; margin-bottom: 10px;">6. Generating the Stylized Frames</h2>
              <p>With the output from previous modules, we use KSampler to generate the stylized frames based on the conditioned model and prompts.</p>
            </div>
        
            <div style="margin-bottom: 20px;">
              <h2 style="color:  #7f8c8d; font-size: 1.3em; margin-bottom: 10px;">7. Enhancing the Images with FreeU2</h2>
              <p>After receiving outputs from KSampler, we apply FreeU2 to enhance the generated frames, making them clearer and more detailed.</p>
            </div>
        
            <div style="margin-bottom: 20px;">
              <h2 style="color:  #7f8c8d; font-size: 1.3em; margin-bottom: 10px;">8. Assembling the Final Video</h2>
              <p>All processed frames are combined to create the final stylized video.</p>
            </div>
          <img src="static/images/wf2.png">
          <p>
            In addition to our custom ComfyUI setup, we also used <a href="lensgo.ai">Lensgo.ai</a> for some of the sections compute time was running out.
          </p>
          </div>
        </div>
        
      </div>
    </section>


    
    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h4 class="title is-2">Style Transfer Results</h4>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
              <!-- Your image here -->
              <video poster="" id="tree" controls muted loop height="100%">
                <!-- Your video here -->
                <source src="static/videos/atlas_atlas_sequence.mp4" type="video/mp4">
              </video>

              <h2 class="subtitle has-text-centered">
                Input (Amber dancing) 
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <video poster="" id="tree" controls muted loop height="100%">
                <!-- Your video here -->
                <source src="static/videos/atlas-gen-v1.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                Output (Local Model)
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <video poster="" id="tree" controls muted loop height="100%">
                <!-- Your video here -->
                <source src="static/videos/icarus1_sequence.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                Input (Amber dancing)
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <video poster="" id="tree" controls muted loop height="100%">
                <!-- Your video here -->
                <source src="static/videos/icarus-gen-v1.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                Output (local model). 
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <video poster="" id="tree" controls muted loop height="100%">
                <!-- Your video here -->
                <source src="static/videos/icarus-gen-v2.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                Second output (local model)
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <video poster="" id="tree" controls muted loop height="100%">
                <!-- Your video here -->
                <source src="static/videos/Sys2.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                Input (Amber stretching)
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <video poster="" id="tree" controls muted loop height="100%">
                <!-- Your video here -->
                <source src="static/videos/Sys2_c.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                Output (Lensgo.ai) - Ghibli Style (with image attached), Emotional, Woman, Purple Hair, Stretching
              </h2>
            </div>
      

          </div>

          <p>
          </p>
        </div>
      </div>
    </section>

    <section class="hero is-small">
      <div class="hero-body">
        <div class="container expdiv">
          <h4 class="title is-2">Post Production</h4>
          <p>
            For the post-production of our music video, we utilized Adobe Premiere Pro as our primary editing platform. Central to this editing process was honoring the flow of the music. We tried to strike a balance between enhancing the musical elements as well as telling a narrative that, although aligned with the theme of the music, tells a slightly different story. Where the music tells the story of a battle between humans and AI, the video tells the story of the AI having a position of power over the humans, defeating them in the end. This balance was mostly upheld by editing in such a way that musical elements were underlined by for example switching between images on the beat, or highlighting key musical details by animating certain elements so that the focal point of the video aligned with those moments. This way we aimed to create a connection between auditory and visual components of the music video. 
	Our editing process was mostly iterative. We started compiling images and videos before the entirety of the plot was decided on. This way we were able to adapt when we were not able to generate a certain fragment we had in mind or change the narrative slightly if the AI tools we were using generated something that was different from what we originally expected to work with. We believe that this dynamic process was integral to the final end product. Before we started working on the project we decided not to let the AI generate everything, but rather see how we can incorporate AI elements within our own creative process. This way we aimed to achieve true collaboration between ourselves and AI tools. 
          </p>
          <img src="static/images/moreimgs.JPG" alt="atlas"/>

          <p>
          </p>
        </div>
      </div>
    </section>


    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h4 class="title is-2">Analysis and Interpretation</h4>
          <p>
            In this section, we will provide an analysis of our video, to highlight individual components in detail, as well as narrative structure and stylistic choices. By breaking this down, we aim to explain how all these elements come together to convey the intended story, aesthetic and message. 
            <br>
            The music video is built up of five sequences, each telling a part of the full story. The overall story is a cautionary tale describing the journey of a human tired of their daily struggle, trusting in technology so much that it leads to their own demise. Essentially the video serves as a warning of responsible use of generative AI.
          </p>
          <div>
            <div class="expdiv">
              <!-- Your image here -->
              <img src="static/images/atlas.png" alt="Globe" />

              <h2 class="subtitle has-text-centered">
                The Atlas Sequence 0:00 - 0:41
              </h2>
              <p>In this very first sequence, we are presented with a display of nature. The final element of this sequence is Atlas, the carrier of the world. This sequence serves as an introduction to elements that will be explored further in the next sequences, such as the purple flower, green nature, the world and Greek mythological figures.</p>
            </div>
            <div class="expdiv">
              <!-- Your image here -->
              <img src="static/images/sys_globe.png" alt="atlas" class="img2 "/>
              <h2 class="subtitle has-text-centered">
                The Sysiphus sequence - 0:41 - 1:25
              </h2>
              <p>The viewer is confronted with a representation of the human struggle. Everyday the human tries to push the world up the mountain and every day the world comes right back down. She grows frustrated with this process and wishes for something or someone to ease her struggle. "One must imagine Sisyphus happy" they say, yet this human, as many among us, does not wish to suffer and would take any chance at making life just that little bit easier. 
              </p>
            </div>
            <div class="expdiv">
              <!-- Your image here -->
              <img src="static/images/grimesSequ.png" alt="Woman from back" />
              <h2 class="subtitle has-text-centered">
                The Grimes sequence - 1:25 - 1:56
              </h2>
              <p>Named after the featured singer, the Grimes sequence sees the human´s wish fulfilled. A magical glowing orb comes down to earth and the human is equally as intrigued as excited to take whatever the orb might bring. The purple orb here represents the gift of AI that is bestowed upon planet earth. 
              </p>
            </div>
            <div class="expdiv">
              <!-- Your image here -->
              <img src="static/images/Icarus.png" alt="Woman from back" />
              <h2 class="subtitle has-text-centered">
                The Icarus sequence part 1 - 1:56 - 2:30

              </h2>
              <p>
                The human undergoes a transformation as an effect of taking the purple orb. She grows wings and decides to tie the earth to a rope so she can take the earth up high in the sky. She prepares for takeoff. Her struggle is to be greatly diminished with this new tool. 
              </p>
            </div>
            <div class="expdiv">
              <!-- Your image here -->
              <img src="static/images/Icarus2.png" alt="Woman from back" />
              <h2 class="subtitle has-text-centered">
                The Icarus sequence part 2 - 2:30 - 3:14
              </h2>
              <p>
                The human flies high up into the sky with the earth. The AI however decides to make the human aware of her own mortality and blinds her, leading her to plummet into the now purple ocean. AI has taken over the world and humans alike, quite literally holding the world in its hand. The purple flower remains glowing, yet is the only plant left alive. 
              </p>
            </div>
            <div class="expdiv">
              <!-- Your image here -->
              <img src="static/images/color-progression.JPG" alt="Green mixed violet" />
              <h2 class="subtitle has-text-centered">
                Colors
              </h2>
              <p>
                A main theme throughout the video is the use of color, as a way to visually juxtapose the natural to the AI. Throughout the course of the video, much of the visual elements turn from green to purple. This is one of the ways we represent the AI taking over. 
              </p>
            </div>
            <div class="expdiv">
              <!-- Your image here -->
              <img src="static/images/flowers.JPG" alt="Flower" />
              <h2 class="subtitle has-text-centered">
                The Flower
              </h2>
              <p>
                A recurring theme within the video is that of the purple flower. The purple flower represents the existence of AI within the natural world. It's a growing flower, blooming within the boundaries of the world humans live in as well. The lush green nature around it represents, as did the color green throughout the video, the thriving of nature and humans alike. The flower scene occurs three times throughout the video in total. The first two times we see it, the trees and plants around it are still blooming, AI exists safely within the boundaries of our world. The third time we see it though, AI has taken over and everything around the ever glowing purple flower has died. 
              </p>
            </div>
            <div class="expdiv">
              <!-- Your image here -->
              <img src="static/images/globe.JPG" alt="Flower" />
              <h2 class="subtitle has-text-centered">
                The World
              </h2>
              <p>
                The final recurring theme within the video is that of the world. It represents the responsibility that humans to handle the challenges of life.  Throughout the story, we see this daily human effort to do so. In the end humans loose control  in their efforts to relieve themselves of their toil
                </p>
            </div>
          </div>
        </div>
      </div>
    </section>



    <!-- Youtube video -->
    <section class="hero is-small is-light" id="videoshow">
      <div class="hero-body">
        <div class="container">
          <!-- Paper video. -->
          <h2 class="title is-3">The Final Video</h2>
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">

              <div class="publication-video">
                <!-- Youtube embed code here -->
                <iframe src="https://www.youtube.com/embed/6VUHeif5fzw?si=5KHXjDx77HZlZCvi" frameborder="0"
                  allow="autoplay; encrypted-media" allowfullscreen></iframe>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End youtube video -->








    <!-- Paper poster -->
    <section class="hero is-small is-light">
      <div class="hero-body">
        <div class="container">
          <h2 class="title">Task Devision</h2>

          <p>Efraim Dahl - Everything Music and Sound (Music Generation, Timbre Transfer, Singing, Lyrics, Production), Project Page, Cameraman</p>
          <p>Amber Koelfat - Video Director, Editor, Image + Video Generation, Story+Analysis, Dancing, Choreography</p>
          <p>Timofey Senchenko - Image + Video Generation, Video Style Transfer - Comfy UI Workflow, Cloud Computation Setup</p>

        </div>
      </div>
    </section>
    

    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">

              <p>
                This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank">Academic Project Page Template</a> which was adopted from the <a
                  href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              </p>
              <p>
                Attributions:<br> 
                Abdou: Abdou aka Mathiavelly, is a friend and collaborator of Efraim from Dakar, Senegal. Check out his <a href="https://www.youtube.com/@capsiboymathiavelly">Youtube Channel</a><br>
                Dark Spoken Word Female Vocal Sample by Sample_Me -- https://freesound.org/s/610048/ --
                License: Creative Commons 0<br>
                Ode to Joy Horn Sample: Utah Symphony Associate Principal Horn Edmund Rollett,  https://www.youtube.com/watch?v=HuxANPsnFoE<br>

              </p>
              

            </div>
          </div>
        </div>
      </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->
    <script
      src="https://cdn.jsdelivr.net/combine/npm/tone@14.7.58,npm/@magenta/music@1.23.1/es6/core.js,npm/focus-visible@5,npm/html-midi-player@1.5.0"></script>
    <script src="js/waveCanvas.js"></script>
</body>

</html>