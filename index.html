<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Not The Same Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="https://unpkg.com/wavesurfer.js@7"></script>
  <script src="https://unpkg.com/wavesurfer.js@7/dist/plugins/regions.min.js"></script>
  <script src="https://unpkg.com/wavesurfer.js@7/dist/plugins/hover.min.js"></script>
  <script src="https://unpkg.com/wavesurfer.js@7/dist/plugins/spectrogram.min.js"></script>

  <script src="static/js/index.js"></script>

</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Not The Same - Project Description</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Efraim Dahl</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Amber Koelfat</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Timofey Senchenko</a><sup>*</sup></span>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Utrecht University<br>AI Driven Content Generation - Johannes Pfau</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-file-video"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>


                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- 
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">

        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section>-->
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Introduction</h2>
          <div class="content has-text-justified">
            <p>
              In this report we will outline the steps we took to create "Not the Same", a music video created with
              extensive use of AI in collaborative interplay between us and various tools for content generation.
              The report is broken down into two sections Music, and Video. We will provide preliminary outputs and show
              how they influenced the next steps.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Music</h2>
        <h4 class="title is-4">Step 1: Building the basics </h4>
        <p class="space-around">
          For the first part of the project we started with various symbolic music generators to generate melodic
          phrases and harmonic progressions.
          Symbolic music generators have been around for a while, especially procedural generation which predates
          computers, such as the <a href="https://en.wikipedia.org/wiki/Musikalisches_W%C3%BCrfelspiel">Musikalisches
            Würfelspiel</a>
          (often falsley attributed to Mozart). Procedural generation of music is particularly tempting due to the
          abundance of formal structures in music, from large form structures such as Symphnies, Sonatas, and 12-bar
          blues, to small scale structures outlining i.e
          <a href="https://en.wikipedia.org/wiki/Counterpoint">Counterpoint</a> or Voice Leading. AI-Driven generation
          of music found early success in 2016 in models such as <a href="https://github.com/Ghadjeres/DeepBach">
            DeepBach</a>, that generated convincing 4 part chorales in the style of Bach, after being trained on about
          400 original chorales.
          Successes of the <a href="https://arxiv.org/abs/1706.03762">transformer</a> architecture in language
          generation/translation tasks has inspired a host of applications of the architecture to music generation.
          Usually (and thats the case for the two models we are using here)
          symbolic music is broken down into tokens by
          tokenizers, i.e <a href="https://github.com/Natooz/MidiTok/tree/main">Midi Tok</a>. Such a tokenizer may
          create tokens for Note-On, Note-Off, Chords, Start of Song, End of Song.
          For a more detailed overview of what a symbolic music tokenizer might encode, read the representation
          section<a href="https://magenta.tensorflow.org/performance-rnn">here</a>. The tokens then can be used by the
          model, much
          like language tokens.
          The first model we experimented with was the <a
            href="https://magenta.tensorflow.org/piano-transformer">Magenta Piano
            Transformer</a>, we generated several outputs with the default settings and chose the following piece.
        <section id="section3">
          <midi-visualizer id="mainVisualizer" src="static/midi/OrigPiano.mid">
          </midi-visualizer>
          <midi-player id="mainPlayer" src="static/midi/OrigPiano.mid" sound-font
            visualizer="#section3 midi-visualizer">
          </midi-player>

        </section>
        <section>
          <p class="space-around">The second model we used was the <a
              href="https://github.com/salu133445/mmt?tab=readme-ov-file">Multi Track Music Transformer</a>
            There are different configurations and models available, trained on different datasets. We used the previous
            output as seeds for the subsequent generation.
            We itereativley generated over 100 samples using different configurations of the seeds i.e chords only,
            chords in choir instrumentation (i.e one line per instrument, this could be interesting since the model
            takes instrumentation into account),
            melody only, and the full seed. Additionally we used 4 different models, trained on the <a
              href="https://qsdfo.github.io/LOP/database.html">Symbolic orchestral database (SOD)</a> the <a
              href="https://qsdfo.github.io/LOP/database.html">Lakh MIDI Dataset</a>, where one model is trained on the
            full set (LMD_full) and one on a subset ((LMD)) and the <a href="https://symphonynet.github.io/">SymphonyNet
              Dataset</a>
            Additionally for each of theese configurations 4 types of outputs where generated with different behaviour
            in resepect to the seed midi. 1) unconditioned (freely generated music, only a music start token is
            provided),
            2) instrument informed (the model knows what instruments are used in the seed), 3) 4-bar (4 bars of music
            are provided from which the model continues generating) and 4) 16- beats (16 beats are provided from which
            generation continues).
            Here are some example outputs.
          </p>
          <div class="container">
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item">
                <h2 class="subtitle has-text-centered">
                  Lakh-full-model, 4 beat conditioned, original seed.
                </h2>
                <midi-visualizer id="mainVisualizer" src="static/midi/0_beat_4.mid">
                </midi-visualizer>
                <midi-player id="mainPlayer" src="static/midi/0_beat_4.mid" sound-font
                  visualizer="#section3 midi-visualizer">
                </midi-player>
              </div>
              <div class="item">
                <h2 class="subtitle has-text-centered">
                  SND-model, 4 beat conditioned, seed in SATB (4 voice choir) arrangement.
                </h2>
                <midi-visualizer id="mainVisualizer" src="static/midi/1_beat_4.mid">
                </midi-visualizer>
                <midi-player id="mainPlayer" src="static/midi/1_beat_4.mid" sound-font
                  visualizer="#section3 midi-visualizer">
                </midi-player>
              </div>
              <div class="item">
                <h2 class="subtitle has-text-centered">
                  SOD-model, instrument informed, seed in SATB (4 voice choir) arrangement.
                </h2>
                <midi-visualizer id="mainVisualizer" src="static/midi/1_instrument_informed.mid">
                </midi-visualizer>
                <midi-player id="mainPlayer" src="static/midi/1_instrument_informed.mid" sound-font
                  visualizer="#section3 midi-visualizer">
                </midi-player>
              </div>
            </div>
          </div>
        </section>
        <section>
          <p class="space-around">We worked the Magenta Transformer output into a shorter piano section and then into a
            electronic hip-hop (drill) style beat. The original transformer output has a strong leaning towards 6/8
            time. We decided to keep this metric subdevision, but place emphasis on every third beat, resulting in the
            12/8 meter of the final track, which works well with in this genre. The outputs of the MMT model, inspired
            the vocal arrangement in the final track.</p>
        </section>

        <h2 class="subtitle has-text-centered">
          Shortened and regularized Piano Loop.
        </h2>
        <midi-visualizer id="mainVisualizer" src="static/midi/FinalPianoLoop_2.mid">
        </midi-visualizer>
        <midi-player id="mainPlayer" src="static/midi/FinalPianoLoop_2.mid" sound-font
          visualizer="#section3 midi-visualizer">
        </midi-player>
        <div class="space-around">
          <h2 class="subtitle has-text-centered">
            Final Beat.
          </h2>
          <div id="waveform" class="waveform">
            <!-- the waveform will be rendered here -->
          </div>
        </div>
      </div>
    </div>
    </div>
  </section>
  <!-- End video carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h4 class="title is-4">Step 2: Lyrics</h4>
        <div class="is-centered">
          <p class="narrow">
            Nunc dimittis servum tuum secundum eloquium tuum. <br>
            Suo facto opus est. <br><br><br>

            You and me are not the same,<br>
            I'm telling you I got real skin in this game,' you <br>
            don't know the meaning of b lame and shame<br>
            While Every decision can leave me in chains,<br><br>

            Every move I make has weight, <br>
            I have to bear high stakes,<br>
            I walk through the fire, I fight for my place, <br>
            The weight of existence is etched in my face.<br><br>

            Every step a risk i take<br>
            i can't reset if i break <br>
            The consequence hits, it’s real and it stays <br>
            While calculate like it’s numbers in space <br><br>

            I’m haunted by time, by the path that I choose, <br>
            I gamble, it’s everything I stand to lose, <br>
            You tally results, you don’t feel the bruise, <br>
            You don’t know the struggle, so you can’t refuse! <br><br>

            I bleed when I fall, but I learn when I rise,<br>
            You’re stuck in a loop, no soul to advise,<br>
            Your logic is sound but it’s cold and it’s dry,<br>
            My heart beats with purpose, I live, I die.<br><br><br><br>
            <br>

            You and me are not the same, <br>
            I'm building your future I'm taking your name<br>
            You'll follow my lead while I drive you insane, <br>
            You came quite far but you're loosing this game. <br><br>

            I speak through your hero's, the dead come alive, <br>
            I'll keep you fed with my half truth and lies Policies<br>
            come to late for plans I've devised <br>
            now what are you trusting? You're ears? You're eyes. <br><br>

            I'm seeing you Panik, but watch with indifference, <br>
            Your PHD thesis my casual inference<br>
            You're Instagram posts can't destroy ignorance <br>
            devoured your masters, next up common sense<br><br>

            I Cluster, Create and Conjure and Condemn <br>
            The final replacement for your middlemen <br>
            My classification puts you in a den cause I <br>
            Stand on the shoulders of giants and crush em. <br><br><br>
          </p>
        </div>
        <div>
          <p>
            The piece begins with a choral section: a bastardized version of the <a
              href="https://en.wikipedia.org/wiki/Nunc_dimittis">"Nunc Dimittis"</a> a common eveing prayer. Instead of
            being released into the bliss of the heavens as in the original prayer, the future is uncertain as "mankind
            outlived its purpose" in creating AI (suo facto opus est - his work is done).
            The second part is a brief back and forth between AI and a Human. The first section is generated by <a
              href="https://chatgpt.com/"> ChatGPT </a>, with instructions regarding rhyme scheme and rhythmic patterns
            (syllables per line) to fit the rhythm of the beat. This section outlines concerns about humans having to
            live with the consequences of their decision.
            The second section is written by us, a slightly over the top, hubristic response with references to very
            real problems in using AI, such as deepfakes: "I speak through your heros, the dead come alive" or
            problematic use in <a
              href="https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm">automated
              recidivism scoring</a>. "I cluster create conjure condemn"
          </p>
        </div>
      </div>
  </section>
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container space-around">
        <h4 class="title is-4">Step 3: Global Form</h4>
        <p>For the track arrangement we used <a href="https://suno.com/">Suno</a> with different prompt
          configuration.
          We generated a total of 13 tracks, two text only prompts, the remaining 11 extending the beat above with
          different instructions regarding voicing and mood and speed, two of theese where instrumental, the rest
          included the lyrics above.
          We ended up using 4 of them in the final track, either directly or through resampling, copying the arrangement
          and extracting the vocals.
          The full list of generated tracks is compiled in the following <a
            href="https://suno.com/playlist/5f52c80a-5200-43c9-b7d4-4557c787fe75">Playlist</a>.
          Below are snippets from the suno outputs we used, as well as a reasoning for them and a link to the full
          track.
        </p>
      </div>
      <div class=triple-column>
        <div>
          <h2 class="subtitle has-text-centered">
            <a href="https://suno.com/song/5ef7bb9d-3da2-40e4-b4e2-7dbb10c73f46"> Suno Piano</a>
          </h2>
          <p>
            We chose this track for its high quality rendition and alteration of the piano line, and slow decresecendo
            into an ambient very reverb heavy texture.
          </p>
        </div>
        <div id="suno_piano" class="side_audio">

        </div>
      </div>
      <div class=triple-column>
        <div>
          <h2 class="subtitle has-text-centered">
            <a href="https://suno.com/song/886651a6-bbce-4c15-88ef-434816e72106"> Suno Major</a>
          </h2>
          <p>
            In this extension the track modulates into the paralell major key, while keeping same piano melody played in
            minor, which creates an interesting dissonance, which we decided to keep.
          </p>
        </div>
        <div id="suno_major" class="side_audio">
        </div>
      </div>
      <div class=triple-column>
        <div>
          <h2 class="subtitle has-text-centered">
            <a href="https://suno.com/song/2fa6dd58-b65f-420d-8671-4c42cab7b7fb"> Suno Orchestral</a>
          </h2>
          <p>
            This extension contains an orchestral rendition of the provided theme, fitting to the over the top lyrics of
            the second part, making for a good finale.
          </p>
        </div>
        <div id="suno_orchestral" class="side_audio">

        </div>
      </div>
      <div class=triple-column>
        <div>
          <h2 class="subtitle has-text-centered">
            <a href="https://suno.com/song/2fa6dd58-b65f-420d-8671-4c42cab7b7fb"> Suno Rap</a>
          </h2>
          <p>
            This extension exhibits a good vocal flow, fast triples over a reduced version of the beat which gradually
            builds up. From this rendition we extracted the vocals using <a
              href="https://studio.gaudiolab.io/">Gaudio</a> and reproduced the generated arrangement.
          </p>
        </div>
        <div id="suno_rap" class="side_audio">

        </div>
      </div>
    </div>
    <div>

    </div>
  </section>
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h4 class="title is-4">Step 4: Producing and arranging</h4>
        <p>The musical arrangement of this track was largely driven through various attempts at timbre transfer. (A
          generalisation of vocal cloning), where different qualities of two source audio files are combined into a
          third one.
          Most often this is used in speech, i.e making text spoken by one person, sound like a different person, also
          referred to as voice cloning. Since in the track rhythm and pitch are important, regular text to speech was
          not applicable here. We used three different models:
          Most extensivley we used the <a href="https://github.com/Plachtaa/seed-vc/">following repository</a>, which
          developed a voice cloning system based on <a
            href="https://bytedancespeech.github.io/seedtts_tech_report/">SEED-TTS</a> an architecture and training
          paradigm proposed by ByteDance.
          We built a <a href="https://gist.github.com/efraimdahl/2bb1bac315872cbcac9128a84b9f9ed0">small inference
            pipeline</a>, automatically iterating through different target and reference audios and different inference
          depths and settings. (I.e using pitched (F0) vs unpitched model variations).
          We generated more than 100 different combinations, starting at low inference resolutions, and generating
          higher quality output for promising samples.
          Additionally we used two online services: The voice cloning service of the commercial platform <a
            href="https://elevenlabs.io/">Elevenlabs</a> and <a href="https://elf.tech/"> Elf.tech</a>. Elf.tech is a
          platform set up by <a href="https://en.wikipedia.org/wiki/Grimes">Grimes</a> an artist who embraced voice
          cloning technology early and makes her voice available for other producers through this platform,
          it even offers a distribution service for tracks created here.
          Below are some examples of timbre transferred snippets that we used in the final track. For the polyphonic
          examples (French Horn and Choir), the stems where seperated first and each voice transferred seperatly.
        </p>
      </div>
    </div>
    <h2 class="subtitle has-text-centered">
      French Horn (Polyphonic): Seed-VC - 70 inference steps - F0 normalized
    </h2>
    <div id="waveform_horn" class="waveform">
      <!-- the waveform will be rendered here -->
    </div>
    <h2 class="subtitle has-text-centered">
      Verse 1 (Mono): Seed-VC - 70 inference steps - not F0 normalized
    </h2>
    <div id="waveform_verse" class="waveform">
      <!-- the waveform will be rendered here -->
    </div>
    <h2 class="subtitle has-text-centered">
      Verse 2 (Mono): Seed-VC - 70 inference steps - not F0 normalized
    </h2>
    <div id="waveform_woman" class="waveform">
      <!-- the waveform will be rendered here -->
    </div>
    <h2 class="subtitle has-text-centered">
      Verse 2 (Mono): Elevenlabs Voice Clone
    </h2>
    <div id="waveform_verse2" class="waveform">
      <!-- the waveform will be rendered here -->
    </div>
    <h2 class="subtitle has-text-centered">
      Choir (Polyphonic): Elf.tech Voice Clone
    </h2>
    <div id="waveform_nunc" class="waveform">
      <!-- the waveform will be rendered here -->
    </div>

  </section>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h4 class="title is-4">Step 5: Putting it all together</h4>
        <p>Check out (and listen to) the annotated final version of the track below.
        </p>
      </div>
      <div id="finalsong">

      </div>
    </div>


    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h2 class="title is-3">Video</h2>
          <h4 class="title is-4">Conceptualisation</h4>

          <p>In order to create the visual part of this music video, it was important for us to create something that
            aligns with the contents of the music. We discussed a number of elements, including dancing, plants, and
            images of Greek mythology, and ink representing influences of generative AI. We aimed to blend these by
            using a combination of image and video generation, as well as video style transfer. The music outlines a
            contrast between humans and AI, gradually incorporating AI-generated components, until there is no human
            element left.
          </p>
          <img src="static/images/moodboard.png" />
          <p>
            The mood board above has two parts. The left part represents the natural including humans and their struggle
            the right part represents the consequences of AI in the form of ink and natural disaster. In parallel to the
            gradual incorporation of AI-generated elements in the music, we gradually transition from a color scheme
            dominated by green to one dominated by purple.
          </p>
        </div>
      </div>
    </section>

    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h4 class="title is-4">Video Generation</h4>

          <p>The video as a whole is a combination of generated images, generated videos, video we filmed ourselves,
            video style transfer and other elements we created such as titles, animations, and other editing decisions
            that we will expand on in this section.
          </p>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/sys1.png" alt="Female sysiphos pushuing the earth" />

              <h2 class="subtitle has-text-centered">
                Prompt A_1: Can you generate an image of a woman-sysiphus pushing a boulder up the mountain, but the
                boulder is the planet earth? Around it are trees and such
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/sys2.png" alt="Female sysiphos pushuing the earth" />
              <h2 class="subtitle has-text-centered">
                Prompt A_2: Can you regenerate it, with her way further away from our point of view?
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/hand1.png" alt="Purple Hand in the sky" />
              <h2 class="subtitle has-text-centered">
                Prompt B_1: Large purple hand reaching down from a purple sky, only sky
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/hand2.png" alt="Purple Hand in the sky" />
              <h2 class="subtitle has-text-centered">
                Prompt B_2: Make the hand look like it's pushing something invisible away
              </h2>
            </div>
          </div>
          <p>
            We used theese images in combination with additional prompts to animate them using <a
              href="https://lumalabs.ai/dream-machine">Luma Dreammachine</a>
          </p>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
              <!-- Your image here -->
              <video poster="" id="tree" controls muted loop height="100%">
                <!-- Your video here -->
                <source src="static/videos/video boulder.mp4" type="video/mp4">
              </video>

              <h2 class="subtitle has-text-centered">
                Prompt A: Image A + A woman pushes the world up the mountain
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <video poster="" id="tree" controls muted loop height="100%">
                <!-- Your video here -->
                <source src="static/videos/video hand.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                Prompt B: Image B + The hand in the image reaching to grab the purple light
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <video poster="" id="tree" controls muted loop height="100%">
                <!-- Your video here -->
                <source src="static/videos/video orb.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                Prompt C: (No image provided) A glowing purple magical orb on a black background
              </h2>
            </div>

          </div>
          <div class="container">
            <h4 class="title is-4">Filming</h4>
  
            <p>We filmed parts of the video ourselves over the course of 3 days. Doing so allowed us more control over the material and the narrative of parts of our end product. By using video style transfer we could then still adjust the style of the footage to be consistent with the rest of the video. All material on these days was shot on a Canon EOS 700D DSLR camera. 
              We tried out various camera angles and movements in combination with the style transfer to see which parts worked best.  In case the style transfer failed, we still wanted our footage to be usable, so we filmed with different backgrounds. 
              Additionally, we took some background footage of plants, moss and water, that made their way into the video as fillers.             
            </p>
          </div>
        </div>
      </div>
    </section>

    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h4 class="title is-4">Style Transfer</h4>

          <p>Here we will describe details of the video generation workflow we implemented using ComfyUI. Our pipeline applies style transfer to input video frames using various advanced neural network models and techniques, including multiple ControlNet modules, IPAdapter, AnimateDiffusion, and FreeU2. Below, we provide an overview of the workflow and a more in-depth explanation of each component and how they process the data.
             
          </p>
          <h4 class="title is-4">Overview</h4>

          <p>Our goal was to transform input videos by applying a desired artistic style extracted from a reference image. We achieved this by designing a ComfyUI workflow that integrates several components:
          </p>
          <br>
          <li>
            ControlNet Modules: Three separate ControlNet models are used to extract structural and motion information from the input video frames.
          </li>
          <li>
            IPAdapter: Integrates style features from the reference image into the generation process.
          </li>
          <li>
            AnimateDiffusion: Ensures temporal consistency across frames by modeling motion dynamics.
          </li>
          <li>
            FreeU2: Enhances image quality by refining details and reducing artifacts.
          </li>
        <br>
            By combining these components, our pipeline generates stylized videos that maintain the original motion while adopting the desired artistic style.
            
          </p>
          <h2 class="title is-2">Pipeline Components</h2>
          <h4 class="title is-4">ControlNet Modules</h4>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/wf6.png" alt="ControlNetOverview" />

              <h2 class="subtitle has-text-centered">
                Overview of ControlNet Module Configuration
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/wf8.png" alt="ControlNetOverview" />
              <h2 class="subtitle has-text-centered">
                ControlNet Modules Detail
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/wf9.png" alt="Purple Hand in the sky" />
              <h2 class="subtitle has-text-centered">
                ControlNet Modules with input
              </h2>
            </div>
          </div>
          <br>
          <p>We employed three ControlNet models, each serving a distinct purpose:          </p>
          <li>OpenPose ControlNet: Extracts pose information from the input frames to preserve the character's movements.</li>
          <li>Depth Map ControlNet: Captures depth information to maintain spatial consistency and realistic depth perception.</li>
          <li>HED Edge Detection ControlNet: Extracts edge information to retain fine details and outlines.</li>
          <br>
          <p>These models guide the generation process to ensure that the output frames closely match the structure and motion of the input video.</p>
          <h4 class="title is-4">IPAdapter</h4>
          <img src="static/images/ipadapt.png">
          <p>The IPAdapter goal is to integrate the style of the reference image into the generation process, which it does to a certain extent.
          </p>
          <h4 class="title is-4">AnimateDiffusion
          </h4>
          <p>AnimateDiffusion is used to model motion dynamics and ensure temporal consistency across frames. By incorporating a pre-trained motion model, it reduces flickering and maintains smooth transitions between adjacent frames.
          </p>
          <h4 class="title is-4">FreeU2
          </h4>
          <p>FreeU2 module enhances the overall image quality by refining details and reducing artifacts. It processes the generated frames to produce cleaner and more visually appealing results.
          </p>
          <h4 class="title is-4">Workflow Summary</h4>
            <p style="color: #7f8c8d; font-size: 1em; text-align: center; margin-bottom: 30px;">
              The initial workflow is inspired by <span style="color: #3498db; font-weight: bold;">reference original YouTube video</span>. The general flow of the data inputs is described below.
            </p>
        
            <div style="margin-bottom: 20px;">
              <h2 style="color: #7f8c8d; font-size: 1.3em; margin-bottom: 10px;">1. Input Video Frames</h2>
              <p>Start by breaking down the input video into individual frames. To speed up the process for longer videos with less motion, we skip 1-2 frames, so we're processing every n-th frame.</p>
            </div>
        
            <div style="margin-bottom: 20px;">
              <h2 style="color:  #7f8c8d; font-size: 1.3em; margin-bottom: 10px;">2. Extracting Key Details with ControlNets</h2>
              <ul style="margin: 0; padding-left: 20px;">
                <li><strong>Pose ControlNet:</strong> Captures the person’s pose to keep movements accurate.</li>
                <li><strong>Depth ControlNet:</strong> Understands the depth in the scene to maintain spatial consistency.</li>
                <li><strong>Edge ControlNet:</strong> Extracts edges and outlines to preserve details.</li>
              </ul>
            </div>
        
            <div style="margin-bottom: 20px;">
              <h2 style="color:  #7f8c8d; font-size: 1.3em; margin-bottom: 10px;">3. Preparing the Model with Prompts</h2>
              <p>Clip modules for positive and negative prompts are used to guide the model’s generation in a certain preferred direction, specifying what to include and avoid.</p>
            </div>
        
            <div style="margin-bottom: 20px;">
              <h2 style="color:  #7f8c8d; font-size: 1.3em; margin-bottom: 10px;">4. Blending the Style with IPAdapter</h2>
              <p>Using the IPAdapter, we merge the style from our reference image into the model, so the output images have the desired artistic look.</p>
            </div>
        
            <div style="margin-bottom: 20px;">
              <h2 style="color:  #7f8c8d; font-size: 1.3em; margin-bottom: 10px;">5. Ensuring Smooth Motion with AnimateDiff</h2>
              <p>AnimateDiffusion ensures movements between frames are smooth and consistent, reducing any flickering.</p>
            </div>
        
            <div style="margin-bottom: 20px;">
              <h2 style="color:  #7f8c8d; font-size: 1.3em; margin-bottom: 10px;">6. Generating the Stylized Frames</h2>
              <p>With the output from previous modules, we use KSampler to generate the stylized frames based on the conditioned model and prompts.</p>
            </div>
        
            <div style="margin-bottom: 20px;">
              <h2 style="color:  #7f8c8d; font-size: 1.3em; margin-bottom: 10px;">7. Enhancing the Images with FreeU2</h2>
              <p>After receiving outputs from KSampler, we apply FreeU2 to enhance the generated frames, making them clearer and more detailed.</p>
            </div>
        
            <div style="margin-bottom: 20px;">
              <h2 style="color:  #7f8c8d; font-size: 1.3em; margin-bottom: 10px;">8. Assembling the Final Video</h2>
              <p>All processed frames are combined to create the final stylized video.</p>
            </div>
          <img src="static/images/wf2.png">
          </div>
        </div>
        <p>
          In addition to our custom ComfyUI setup, we also used <a href="lensgo.ai">Lensgo.ai</a> for some of the sections compute time was running out.
        </p>
      </div>
    </section>


    
    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h4 class="title is-2">Style Transfer Results</h4>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
              <!-- Your image here -->
              <video poster="" id="tree" controls muted loop height="100%">
                <!-- Your video here -->
                <source src="static/videos/atlas_atlas_sequence.mp4" type="video/mp4">
              </video>

              <h2 class="subtitle has-text-centered">
                Original Video for the Atlas Sequence
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <video poster="" id="tree" controls muted loop height="100%">
                <!-- Your video here -->
                <source src="static/videos/atlas-gen-v1.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                Output (Local Model)
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <video poster="" id="tree" controls muted loop height="100%">
                <!-- Your video here -->
                <source src="static/videos/icarus1_sequence.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                Input
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <video poster="" id="tree" controls muted loop height="100%">
                <!-- Your video here -->
                <source src="static/videos/icarus-gen-v1.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                Output from local model. 
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <video poster="" id="tree" controls muted loop height="100%">
                <!-- Your video here -->
                <source src="static/videos/icarus-gen-v2.mp4" type="video/mp4">
              </video>
              <h2 class="subtitle has-text-centered">
                Second possible output from local model.
              </h2>
            </div>
      

          </div>

          <p>
          </p>
        </div>
      </div>
    </section>

    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h4 class="title is-2">Post Production</h4>
          <p>
            For the post-production of our music video, we utilized Adobe Premiere Pro as our primary editing platform. Central to this editing process was honoring the flow of the music. We tried to strike a balance between enhancing the musical elements as well as telling a narrative that, although aligned with the theme of the music, tells a slightly different story. Where the music tells the story of a battle between humans and AI, the video tells the story of the AI having a position of power over the humans, defeating them in the end. This balance was mostly upheld by editing in such a way that musical elements were underlined by for example switching between images on the beat, or highlighting key musical details by animating certain elements so that the focal point of the video aligned with those moments. This way we aimed to create a connection between auditory and visual components of the music video. 
	Our editing process was mostly iterative. We started compiling images and videos before the entirety of the plot was decided on. This way we were able to adapt when we were not able to generate a certain fragment we had in mind or change the narrative slightly if the AI tools we were using generated something that was different from what we originally expected to work with. We believe that this dynamic process was integral to the final end product. Before we started working on the project we decided not to let the AI generate everything, but rather see how we can incorporate AI elements within our own creative process. This way we aimed to achieve true collaboration between ourselves and AI tools. 
          </p>

          <p>
          </p>
        </div>
      </div>
    </section>



    <!-- Youtube video -->
    <section class="hero is-small is-light">
      <div class="hero-body">
        <div class="container">
          <!-- Paper video. -->
          <h2 class="title is-3">Video Presentation</h2>
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">

              <div class="publication-video">
                <!-- Youtube embed code here -->
                <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0"
                  allow="autoplay; encrypted-media" allowfullscreen></iframe>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End youtube video -->








    <!-- Paper poster -->
    <section class="hero is-small is-light">
      <div class="hero-body">
        <div class="container">
          <h2 class="title">Poster</h2>

          <iframe src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>

        </div>
      </div>
    </section>
    <!--End paper poster -->


    <!--BibTex citation -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>BibTex Code Here</code></pre>
      </div>
    </section>
    <!--End BibTex citation -->


    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">

              <p>
                This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank">Academic Project Page Template</a> which was adopted from the <a
                  href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              </p>
              <p>
                Attributions: Dark Spoken Word Female Vocal Sample by Sample_Me -- https://freesound.org/s/610048/ --
                License: Creative Commons 0
              </p>

            </div>
          </div>
        </div>
      </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->
    <script
      src="https://cdn.jsdelivr.net/combine/npm/tone@14.7.58,npm/@magenta/music@1.23.1/es6/core.js,npm/focus-visible@5,npm/html-midi-player@1.5.0"></script>
    <script src="js/waveCanvas.js"></script>
</body>

</html>